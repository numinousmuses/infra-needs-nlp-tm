{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joshuaokolo/nlp-and-topic-modeling-for-infrastructure-needs?scriptVersionId=104056109\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"from langdetect import detect\nimport re\nemoji_pattern = re.compile(\"[\"\n   u\"\\U0001F600-\\U0001F64F\" # emoticons\n   u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n   u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n   u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n   u\"\\U00002702-\\U000027B0\"\n   u\"\\U000024C2-\\U0001F251\"\n   u\"\\U00002500-\\U00002BEF\" # chinese char\n   u\"\\U0001f921-\\U0001f937\"\n   u\"\\U00010000-\\U0010ffff\"\n   u\"\\u2640-\\u2642\"\n   u\"\\u2600-\\u2B55\"\n   u\"\\u200d\"\n   u\"\\u23cf\"\n   u\"\\u23e9\"\n   u\"\\u231a\"\n   u\"\\ufe0f\" # dingbats\n   u\"\\u3030\"\n   \"]+\", flags=re.UNICODE)\nemail_pattern = re.compile(\"\\S+@\\S+\\.\\S{2,3}\")\nlink_pattern = re.compile(\"https?\\S+\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_data(tweet):\n    try:\n        lang = detect(tweet)\n    if lang == 'en':\n        tweet_rep = emoji_pattern.sub(r'', tweet)\n        tweet_rep = email_pattern.sub(r'', tweet_rep)\n        tweet_rep = link_pattern.sub(r'', tweet_rep)\n        tweet_rep = tweet_rep.replace(\"’\", \"‘\")\n        tweet_rep = tweet_rep.replace(\"&amp;\", \"&\")\n        tweet_rep = tweet_rep.replace(\"#\", '')\n        tweet_rep = tweet_rep.strip()\n        return tweet_rep\n    else:\n        return \"\"\n    except:\n        return \"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Topic Modeling","metadata":{}},{"cell_type":"code","source":"import nltk; nltk.download('stopwords')\nimport re\nimport numpy as npimport pandas as pd\nfrom pprint import pprint# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n# spacy for lemmatization\nimport spacy\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) # deacc=True removes punctuationsdata_words = list(sent_to_words(data))\nprint(data_words[:1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=3, threshold=10) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=10)\n\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)# See trigram example\n\nprint(trigram_mod[bigram_mod[data_words[0]]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\ndef lemmatization(texts, allowed_postags=[‘NOUN’, ‘ADJ’, ‘VERB’, ‘ADV’]):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(“ “.join(sent))\n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n# Form Bigrams\ndata_words_bigrams = make_trigrams(data_words_nostops)\n# Perform lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=[‘NOUN’, ‘ADJ’, ‘VERB’, ‘ADV’])\n\nprint(data_lemmatized[:1])In the lemmatization step, we only keep words that belong to one of the following parts of speech: noun, adjective, verb, adverb. Hence, you can expect to get the output that look as follows:","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Next, we apply the doc2bow function to convert the texts into the bag-of-words (BoW) format, \n# Which is a list of (token_id, token_count) tuples.\n\n# Create a corpus from the lemmatized text we want to analyse\ntexts = data_lemmatized\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n# View\nprint(corpus[:1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LDA Model","metadata":{}},{"cell_type":"code","source":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n            id2word=id2word,\n            num_topics=5,\n            random_state=100,\n            update_every=1,\n            chunksize=50,\n            passes=20,\n            alpha=’auto’,\n            per_word_topics=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lda_model.print_topics())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LDAMallet","metadata":{}},{"cell_type":"code","source":"!curl http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip — output mallet-2.0.8.zip\n!unzip mallet-2.0.8.zip","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mallet_path = ‘/content/mallet-2.0.8/bin/mallet’ # update this path\nldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=5, id2word=id2word)\n\n# To show the topics\nldamallet.show_topics()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optimize number of topics with coherence scores","metadata":{}},{"cell_type":"code","source":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n  \"\"\"\n  Compute c_v coherence for various number of topics\n  Parameters:\n  — — — — —\n  dictionary : Gensim dictionary\n  corpus : Gensim corpus\n  texts : List of input texts\n  limit : Max num of topics\n  Returns:\n  — — — -\n  model_list : List of LDA topic models\n  coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n  \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence=’c_v’)\n        coherence_values.append(coherencemodel.get_coherence())\n    return model_list, coherence_values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=80, step=6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show graph\nlimit=80; start=2; step=6;\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(“Num Topics”)\nplt.ylabel(“Coherence score”)\nplt.legend((“coherence_values”), loc=’best’)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizaiton","metadata":{}},{"cell_type":"code","source":"def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n  # Init output\n  sent_topics_df = pd.DataFrame()\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n    # Get the Dominant topic, Perc Contribution and Keywords for each document\n    for j, (topic_num, prop_topic) in enumerate(row):\n        if j == 0: # => dominant topic\n            wp = ldamodel.show_topic(topic_num)\n            topic_keywords = “, “.join([word for word, prop in wp])\n            sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n        else:\n            break\n    sent_topics_df.columns = [‘Dominant_Topic’, ‘Perc_Contribution’, ‘Topic_Keywords’]\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data)\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = [‘Document_No’, ‘Dominant_Topic’, ‘Topic_Perc_Contrib’, ‘Keywords’, ‘Text’]\n# Show\ndf_dominant_topic.head(20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dominant_topic[‘Dominant_Topic’]==0.0 (use 1.0 for topic 2, 2.0 for topic 3, etc.).\ntopic1data = df_dominant_topic[df_dominant_topic[‘Dominant_Topic’]==0.0]\nkeyw_topic1 = []\n\n    for each in df_dominant_topic['Keywords']:\n        for l in each.strip().split(“,”):\n            keyw_topic1.append(l.strip())\n\nkeyw_topic1 = list(set(keyw_topic1))\nfrom wordcloud import WordCloud, STOPWORDS\n\ndf_dominant_topic[‘Dominant_Topic’].unique()\ntopic1data = df_dominant_topic[df_dominant_topic[‘Dominant_Topic’]==0.0]\ncomment_words = ''\nstopwords = set(STOPWORDS)\n\n# iterate through the csv file\nfor val in topic1data.Text:\n  # typecast each val to string\n  val = str(val)\n  # split the value\n  tokens = val.split()\n  # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n    comment_words += “ “.join(tokens)+” “\n\nwordcloud = WordCloud(width = 800, height = 800,\n                      background_color =’white’,\n                      stopwords = stopwords,\n                      min_font_size = 10).generate(comment_words)\n\n# plot the WordCloud image\nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(“off”)\nplt.tight_layout(pad = 0)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\n\nhttps://radimrehurek.com/gensim/models/ldamodel.html\n\nhttps://github.com/twintproject/twint\n\nhttps://ngrok.com\n\nhttps://pypi.org/project/colabcode/\n\nhttps://omdena.com/blog/infrastructural-needs/\n\nhttps://pypi.org/project/langdetect/","metadata":{}}]}